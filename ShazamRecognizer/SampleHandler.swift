//
//  SampleHandler.swift
//  ShazamRecognizer
//
//  Created by æ°´åŽŸæ¨¹ on 2026/02/15.
//

import ReplayKit
import ReplayKit
import ShazamKit
import AVFoundation
import Accelerate

class SampleHandler: RPBroadcastSampleHandler {
    
    private let signatureGenerator = SHSignatureGenerator()
    private var session: SHSession?
    private var audioBufferCount = 0
    private let appGroupID = "group.media.iccyan.ShazamKit-Practice"
    
    override init() {
        super.init()
        session = SHSession()
        session?.delegate = self
        print("ðŸ“¡ Broadcast Extensionèµ·å‹•")
    }
    
    override func broadcastStarted(withSetupInfo setupInfo: [String : NSObject]?) {
        print("âœ… æ”¾é€é–‹å§‹")
        audioBufferCount = 0
    }
    
    override func broadcastPaused() {
        print("â¸ï¸ æ”¾é€ä¸€æ™‚åœæ­¢")
    }
    
    override func broadcastResumed() {
        print("â–¶ï¸ æ”¾é€å†é–‹")
    }
    
    override func broadcastFinished() {
        print("ðŸ›‘ æ”¾é€çµ‚äº†")
    }
    
    override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType) {
        switch sampleBufferType {
        case RPSampleBufferType.audioApp:
            // ã‚·ã‚¹ãƒ†ãƒ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼ˆSpotifyç­‰ï¼‰
            processAudio(sampleBuffer)
        case RPSampleBufferType.audioMic:
            // ãƒžã‚¤ã‚¯éŸ³å£°ï¼ˆä½¿ã‚ãªã„ï¼‰
            break
        case RPSampleBufferType.video:
            // æ˜ åƒï¼ˆä½¿ã‚ãªã„ï¼‰
            break
        @unknown default:
            break
        }
    }
    
    private func processAudio(_ sampleBuffer: CMSampleBuffer) {
        guard let audioBuffer = sampleBuffer.toAVAudioPCMBuffer() else {
            return
        }
        
        do {
            try signatureGenerator.append(audioBuffer, at: nil)
            audioBufferCount += 1
            
            // 5ç§’ã”ã¨ã«èªè­˜ï¼ˆç´„215ãƒãƒƒãƒ•ã‚¡ï¼‰
            if audioBufferCount >= 215 {
                print("ðŸ” èªè­˜é–‹å§‹... (ãƒãƒƒãƒ•ã‚¡æ•°: \(audioBufferCount))")
                let signature = signatureGenerator.signature()
                session?.match(signature)
                audioBufferCount = 0
            }
        } catch {
            print("âŒ ã‚¨ãƒ©ãƒ¼: \(error)")
        }
    }
}

// MARK: - SHSessionDelegate

extension SampleHandler: SHSessionDelegate {
    func session(_ session: SHSession, didFind match: SHMatch) {
        guard let mediaItem = match.mediaItems.first else { return }
        
        let title = mediaItem.title ?? "ä¸æ˜Žãªã‚¿ã‚¤ãƒˆãƒ«"
        let artist = mediaItem.artist ?? "ä¸æ˜Žãªã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"
        let album = mediaItem.subtitle ?? ""
        
        print("ðŸŽ‰ æ›²ã‚’èªè­˜: \(title) - \(artist)")
        
        // App Groupsã§ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã«é€šçŸ¥
        if let userDefaults = UserDefaults(suiteName: appGroupID) {
            userDefaults.set(title, forKey: "songTitle")
            userDefaults.set(artist, forKey: "songArtist")
            userDefaults.set(album, forKey: "songAlbum")
            userDefaults.set(Date(), forKey: "lastUpdated")
            userDefaults.synchronize()
        }
    }
    
    func session(_ session: SHSession, didNotFindMatchFor signature: SHSignature, error: Error?) {
        if let error = error {
            print("âŒ èªè­˜å¤±æ•—: \(error)")
        } else {
            print("âŒ æ›²ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        }
    }
}

// MARK: - CMSampleBuffer Extension

extension CMSampleBuffer {
    func toAVAudioPCMBuffer() -> AVAudioPCMBuffer? {
        var audioBufferList = AudioBufferList()
        var blockBuffer: CMBlockBuffer?
        
        CMSampleBufferGetAudioBufferListWithRetainedBlockBuffer(
            self,
            bufferListSizeNeededOut: nil,
            bufferListOut: &audioBufferList,
            bufferListSize: MemoryLayout<AudioBufferList>.size,
            blockBufferAllocator: nil,
            blockBufferMemoryAllocator: nil,
            flags: kCMSampleBufferFlag_AudioBufferList_Assure16ByteAlignment,
            blockBufferOut: &blockBuffer
        )
        
        guard let formatDescription = CMSampleBufferGetFormatDescription(self),
              let asbd = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)?.pointee else {
            return nil
        }
        
        guard let format = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: asbd.mSampleRate,
            channels: 1,
            interleaved: false
        ) else {
            return nil
        }
        
        let frameCount = CMSampleBufferGetNumSamples(self)
        
        guard let buffer = AVAudioPCMBuffer(pcmFormat: format,
                                           frameCapacity: AVAudioFrameCount(frameCount)) else {
            return nil
        }
        
        buffer.frameLength = AVAudioFrameCount(frameCount)
        
        guard let dstMono = buffer.floatChannelData?[0] else {
            return nil
        }
        
        if let data = audioBufferList.mBuffers.mData {
            let srcInt16 = data.assumingMemoryBound(to: Int16.self)
            let channelCount = Int(asbd.mChannelsPerFrame)
            
            for i in 0..<frameCount {
                var sample: Float = 0.0
                
                for channel in 0..<channelCount {
                    let int16Value = srcInt16[i * channelCount + channel]
                    sample += Float(int16Value) / 32768.0
                }
                
                dstMono[i] = sample / Float(channelCount)
            }
        }
        
        return buffer
    }
}
